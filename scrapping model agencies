# ___________________OBJECTIF : Constituer une base de données des agences de mannequinat homme à Paris et Londres___________________________________________
"""0 fonctions, 0 classes, c'est volontaire"""

import gspread  # module pour la gestion de l'api google
import requests  # module pour récupérer le code html d'une page
from bs4 import BeautifulSoup  # module pour lire l'html d'une page

# ________Connection à notre gsheet_____________________________________________
sa = gspread.service_account(filename="vici-language-project-1dc3049324f2.json")  # ici on se connecte à l'api google via une clée téléchargée et stocjée dans le json
sh = sa.open("Male model agencies Paris/London")  # ouverture du fichier gsheet
ws = sh.worksheet("Feuille 1")  # ouverture du worksheet 1

# _______Initialisation du scraping_____________________________________________
URL_1 = "https://mannequinat.fr/liste/agences-de-mannequins/?pays=royaume-uni&spe_mannequin=hommes"
URL_2 =  "https://mannequinat.fr/liste/agences-de-mannequins/page/2/?pays=royaume-uni&spe_mannequin=hommes#038;spe_mannequin=hommes"
Input_URLs_list = [URL_1, URL_2]

Output_URLs_list = []  # on va stocker les URL des pages auxquelles on veut accéder pour les scraper par la suite en itérant sur les différentes pages associées chacunes à un unique élément de la data base à rendre

for url in Input_URLs_list:
    page = requests.get(url)
    soup = BeautifulSoup(page.content, "html.parser")
    ugly_listing = soup.find(id="pros_listing")  # on récupère les éléments sous la balise d'ID "pros_listing"
    a_tags_in_ugly_listing = ugly_listing.find_all("a")  # parmis ceux trouvées on accède à ceux qui sont sous la balise a

    for a_tags in a_tags_in_ugly_listing:
        Output_URL_i = a_tags.get("href")  # parmis a_tags_in_ugly_listing on récupère les valeurs href pour accéder aux url des pages uniques qui comportent les données qu'ont veux pour chaque élément de la data base à constituer
        Output_URLs_list.append(Output_URL_i)

# ________Scraping______________________________________________________________

page = requests.get("https://mannequinat.fr/annuaire/3m-models/")
soup = BeautifulSoup(page.content, "html.parser")
agency_name = soup.find("h1", class_="ppro_name ff_serif_ital").string
infos_tag = soup.find("div", class_="ppro_infos").find_all("span", class_="info")
ville_pays = infos_tag[0].next_sibling
adresse = infos_tag[1].next_sibling
phone = infos_tag[2].next_sibling
genres = infos_tag[3].next_sibling

print(agency_name, ville_pays, adresse, phone, genres)
