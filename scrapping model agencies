# ___________________OBJECTIF : Constituer une base de données des agences de mannequinat homme à Paris et Londres___________________________________________

import gspread  # module pour la gestion de l'api google
import requests  # module pour récupérer le code html d'une page
from bs4 import BeautifulSoup  # module pour lire l'html d'une page

# ________Connection à notre gsheet_____________________________________________
sa = gspread.service_account(filename="vici-language-project-1dc3049324f2.json")  # ici on se connecte à l'api google via une clée téléchargée et stocjée dans le json
sh = sa.open("Male model agencies Paris/London")  # ouverture du fichier gsheet
ws = sh.worksheet("Feuille 1")  # ouverture du worksheet 1

# _______Initialisation du scraping_____________________________________________
URL_1 = "https://mannequinat.fr/liste/agences-de-mannequins/?pays=royaume-uni&spe_mannequin=hommes"
URL_2 =  "https://mannequinat.fr/liste/agences-de-mannequins/page/2/?pays=royaume-uni&spe_mannequin=hommes#038;spe_mannequin=hommes"
page = requests.get(URL_1)

soup = BeautifulSoup(page.content, "html.parser")
ugly_listing = soup.find(id="pros_listing")  # on récupère les éléments sous la balise d'ID "pros_listing"
a_tags_in_ugly_listing = ugly_listing.find_all("a")  # parmis ceux trouvées on accède à ceux qui sont sous la balise a


# on va stocker les URL des pages auxquelles on veut accéder pour les scraper en itération
pre_URL_database = []

for a_tags in a_tags_in_ugly_listing:
    URL_i = a_tags.get("href")  # parmis a_tags_in_ugly_listing on récupère les valeurs href pour accéder aux url des pages uniques qui comportent les données qu'ont veux pour chaque élément de la data base à constituer
    pre_URL_database.append(URL_i)

print(pre_URL_database)
